---
title: "Olivia's Portion: Project 2"
author: "Olivia Hofmann"
date: "2024-10-23"
output: pdf_document
---
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Business Understanding

COVID-19 is a highly contagious respiratory illness that first emerged in Wuhan, China in December 2019. COVID-19 entered the United States in January 2020 with the World Health Organization (WHO) declaring COVID-19 a “global health emergency” in March 2020. The virus spreads through respiratory droplets dispersed when someone coughs, sneezes, or even talks. COVID-19 can cause symptoms including those similar to a cold, influenza, or pneumonia with the potential to become very severe and lead to death. The COVID-19 virus overwhelmed healthcare systems and disrupted economies around the world. [1] [2]

The stakeholder for this data analysis is a property developer who is interested in determining the best location in Texas for developing a mixed-use building. The stakeholder’s key concern is selecting a county that demonstrates stability and resilience in response to unpredictable events, like the COVID-19 pandemic. The mixed-use building that the stakeholder is looking to develop will have space for a gym, restaurants, pharmacy, and other similar businesses. When deciding where to build this mixed-use building, the stakeholder is looking for insights into which counties in Texas have successfully managed public health crises as situations similar to this would greatly impact the success of the businesses within his building. Every business that would be in the mixed-use building would be heavily reliant on consistent traffic and economic activity. Any change in foot traffic and economic activity would directly impact the success or failure of each business. The analysis will include data on COVID-19 cases, COVID-19 deaths, and the effectiveness of government interventions (such as lock downs and social distancing). This analysis is crucial for the stakeholder to make an informed decision regarding this long-term investment, as counties that respond well to crises are more likely to provide stable environments for growth and development.

Some questions that the stakeholder would like answered are:

 - What are the characteristics of counties in Texas that showed resilience during the COVID-19 pandemic, based on COVID-19 case rates?
 - What are the economic and social impacts in counties that were more or less affected by the pandemic and how might these influence future development potential?
 - How did COVID-19 impact the workplace and employment rates in the various counties?
 - Which counties showed consistent consumer foot traffic during the pandemic, indicating stable economic activity?

All of these questions are critical because the answers will help the property developer asses the risk and potential returns on his investment. Data needed to complete this analysis includes COVID-19 data for the state of Texas, COVID-19 date for the entire United States, and COVID-19 mobility data for the world. While these datasets seem broad, each dataset contains necessary features to conduct this analysis, which will be revealed further in the report. By understanding how different counties fared during the pandemic, the developer can make an informed decision regarding where he wants to build, ensuring that the chosen location offers stability and growth potential, even during unforeseen circumstances.

## Data Preparation
```{r install packages, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE}
# Suppress all messages and load the libraries.
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(tidyr))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(kableExtra))
suppressPackageStartupMessages(library(factoextra))
suppressPackageStartupMessages(library(ggplot2))
```

```{r install packages, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE}
# Suppress all messages and load the libraries.
pkgs <- c("cluster", "dbscan", "e1071", "factoextra", "fpc", 
          "GGally", "kernlab", "mclust", "mlbench", "scatterpie", 
          "seriation", "tidyverse", "tibble")
  
pkgs_install <- pkgs[!(pkgs %in% installed.packages()[,"Package"])]
if(length(pkgs_install)) install.packages(pkgs_install)
```

```{r load data, echo = FALSE}
# Load the dataset.
dataframe <- read.csv("COVID-19_cases_plus_census.csv")

# Filter the dataset for the counties in Texas.
datatexas <- dataframe %>% 
  filter(state == "TX")

# Only select the desired columns relating to wealth and income. 
wealthincomedata <- datatexas %>% 
  select(county_name,
         confirmed_cases,
         deaths,
         total_pop,
         median_income,
         income_per_capita,
         rent_burden_not_computed,
         rent_over_50_percent,
         rent_40_to_50_percent, 
         rent_35_to_40_percent,
         rent_30_to_35_percent,
         rent_25_to_30_percent, 
         rent_20_to_25_percent,
         rent_15_to_20_percent,
         rent_10_to_15_percent, 
         rent_under_10_percent,
         income_less_10000,	
         income_10000_14999, 
         income_15000_19999,
         income_20000_24999, 
         income_25000_29999, 
         income_30000_34999, 
         income_35000_39999, 
         income_40000_44999, 
         income_45000_49999, 
         income_50000_59999, 
         income_60000_74999, 
         income_75000_99999, 
         income_100000_124999, 
         income_125000_149999, 
         income_150000_199999, 
         income_200000_or_more)
```

### Objects to Cluster
The objects to be clustered in this analysis are the counties in Texas. To identify which counties demonstrated resilience during the COVID-19 pandemic, income and rent burden metrics will be analyzed alongside general population data. Some key features for clustering include median income, income per capita, rent burden levels, and the distribution of income across different brackets. These factors provide a comprehensive picture of each county’s economic resilience and ability to maintain stability during times of crisis.

By examining income distribution and wealth concentration, we can determine which counties have strong economic foundations. This, in combination with COVID-19 case and death data, will guide the stakeholder in making an informed decision on where to invest in developing a mixed-use building. Counties that managed to sustain consumer traffic and economic activity during the pandemic will likely offer more stability and growth potential for future business ventures.

### Features for Clustering
The features analyzed for clustering relate to the category of income and wealth, which are critical for understanding economic resilience. These features include income brackets, median income per capita, rent burden percentages, and population statistics. Each of these features play a significant role in assessing to what capacity the county can withstand a widespread challenge such as the COVID-19 pandemic. 

  - **Income Levels:** The distribution of households across various income levels can provide insight into a county's overall economic health and resilience.
  - **Rent Burden:** High rent burden percentages indicate financial strain on households, which can affect their ability to manage crises effectively.
  - **Median Income and Income per Capita:** These metrics serve as broad indicators of wealth within a county. Wealthier counties typically have more resources to navigate economic shocks and support their communities during difficult times.
  - **Population:** Including population statistics allows for a more accurate interpretation of COVID-19 impacts by normalizing the number of cases and deaths based on county size.

By clustering counties based on these features, we can identify different income and wealth profiles that may correlate with their resilience during the pandemic. This analysis will enhance our understanding of which counties were better equipped to handle the economic and social disruptions caused by COVID-19, ultimately aiding the stakeholder in making informed investment decisions.

### Table of Features and Basic Statistics
```{r basic statistics, echo = FALSE}
# Calculate basic statistics
statstable <- wealthincomedata %>%
  summarise(
    mean_median_income = mean(median_income, na.rm = TRUE),
    sd_median_income = sd(median_income, na.rm = TRUE),
    min_median_income = min(median_income, na.rm = TRUE),
    max_median_income = max(median_income, na.rm = TRUE),
    
    mean_income_per_capita = mean(income_per_capita, na.rm = TRUE),
    sd_income_per_capita = sd(income_per_capita, na.rm = TRUE),
    min_income_per_capita = min(income_per_capita, na.rm = TRUE),
    max_income_per_capita = max(income_per_capita, na.rm = TRUE),
    
    mean_rent_over_50_percent = mean(rent_over_50_percent, na.rm = TRUE),
    sd_rent_over_50_percent = sd(rent_over_50_percent, na.rm = TRUE),
    min_rent_over_50_percent = min(rent_over_50_percent, na.rm = TRUE),
    max_rent_over_50_percent = max(rent_over_50_percent, na.rm = TRUE),
    
    mean_rent_30_to_35_percent = mean(rent_30_to_35_percent, na.rm = TRUE),
    sd_rent_30_to_35_percent = sd(rent_30_to_35_percent, na.rm = TRUE),
    min_rent_30_to_35_percent = min(rent_30_to_35_percent, na.rm = TRUE),
    max_rent_30_to_35_percent = max(rent_30_to_35_percent, na.rm = TRUE),
    
    mean_income_less_10000 = mean(income_less_10000, na.rm = TRUE),
    sd_income_less_10000 = sd(income_less_10000, na.rm = TRUE),
    min_income_less_10000 = min(income_less_10000, na.rm = TRUE),
    max_income_less_10000 = max(income_less_10000, na.rm = TRUE),
    
    mean_income_50000_59999 = mean(income_50000_59999, na.rm = TRUE),
    sd_income_50000_59999 = sd(income_50000_59999, na.rm = TRUE),
    min_income_50000_59999 = min(income_50000_59999, na.rm = TRUE),
    max_income_50000_59999 = max(income_50000_59999, na.rm = TRUE),
    
    mean_income_100000_124999 = mean(income_100000_124999, na.rm = TRUE),
    sd_income_100000_124999 = sd(income_100000_124999, na.rm = TRUE),
    min_income_100000_124999 = min(income_100000_124999, na.rm = TRUE),
    max_income_100000_124999 = max(income_100000_124999, na.rm = TRUE),
    
    mean_total_pop = mean(total_pop, na.rm = TRUE),
    sd_total_pop = sd(total_pop, na.rm = TRUE),
    min_total_pop = min(total_pop, na.rm = TRUE),
    max_total_pop = max(total_pop, na.rm = TRUE)
  )

# Prepare a summary table with descriptions
statstable_final <- data.frame(
  Feature = c(
    "median_income",
    "income_per_capita",
    "rent_over_50_percent",
    "rent_30_to_35_percent",
    "income_less_10000",	
    "income_50000_59999", 
    "income_100000_124999", 
    "total_pop"
  ),
  Description = c(
    "Median income in the county (USD)",
    "Per capita income in the county (USD)",
    "Households with rent > 50% of income (%)",
    "Households with rent 30-35% of income (%)",
    "Households earning <$10,000 (%)",
    "Households earning $50,000-$59,999 (%)",
    "Households earning $100,000-$124,999 (%)",
    "Total population of the county"
  ),
  Mean = c(
    statstable$mean_median_income,
    statstable$mean_income_per_capita,
    statstable$mean_rent_over_50_percent,
    statstable$mean_rent_30_to_35_percent,
    statstable$mean_income_less_10000,
    statstable$mean_income_50000_59999,
    statstable$mean_income_100000_124999,
    statstable$mean_total_pop
  ),
  Std_Dev = c(
    statstable$sd_median_income,
    statstable$sd_income_per_capita,
    statstable$sd_rent_over_50_percent,
    statstable$sd_rent_30_to_35_percent,
    statstable$sd_income_less_10000,
    statstable$sd_income_50000_59999,
    statstable$sd_income_100000_124999,
    statstable$sd_total_pop
  ),
  Min = c(
    statstable$min_median_income,
    statstable$min_income_per_capita,
    statstable$min_rent_over_50_percent,
    statstable$min_rent_30_to_35_percent,
    statstable$min_income_less_10000,
    statstable$min_income_50000_59999,
    statstable$min_income_100000_124999,
    statstable$min_total_pop
  ),
  Max = c(
    statstable$max_median_income,
    statstable$max_income_per_capita,
    statstable$max_rent_over_50_percent,
    statstable$max_rent_30_to_35_percent,
    statstable$max_income_less_10000,
    statstable$max_income_50000_59999,
    statstable$max_income_100000_124999,
    statstable$max_total_pop
  )
)

# Output the table
kable(statstable_final, format = "markdown", caption = "Table of Features and Basic Statistics") %>% 
  kable_styling(full_width = FALSE, font_size = 8.5)

```

Because there are a lot of features that represent the wealth and income category, the basic statistics were done on a subset of the data. Features were chosen that represent the most critical dimensions of income distribution and rent burden, while avoiding overly granular breakdowns. This selection captures the distribution of wealth (from low to high incomes), general population data, and rent burden, which are the most relevant features for analyzing the economic stability of a county.

  - **Median Income:** This gives a central measure of income distribution in a county.
  - **Income per Capita:** Shows wealth distribution on a per-person basis, which complements median income.
  - **Rent Under 10 Percent:**
  - **Rent 25 to 30 Percent:** This provides a threshold of moderate rent burden.
  - **Rent Over 50 Percent:** This is a key indicator of severe rent burden, which can signify economic strain in a county.
  - **Income Less than $10,000:** Reflects the population in extreme poverty, which is crucial for understanding economic vulnerability.
  - **Income $25,000 - $29,999:**
  - **Income $45,000 - $44,999:** 
  - **Income $100,000 - $124,999:**
  - **Income $200,000 or more:**

  
### Scale of Measurement

All of the features listed below are ratio scales because they have a true zero point (e.g., zero income, zero population) and allow for meaningful arithmetic operations (e.g., calculating differences, ratios).

```{r scale of measurement, echo = FALSE}
# Create a data frame with your features and descriptions
features_data <- data.frame(
  Feature = c(
    "median_income",
    "income_per_capita",
    "rent_over_50_percent",
    "rent_30_to_35_percent",
    "income_less_10000",
    "income_50000_59999",
    "income_100000_124999",
    "total_pop"
  ),
  Scale_of_Measurement = c(
    "Ratio",
    "Ratio",
    "Ratio",
    "Ratio",
    "Ratio",
    "Ratio",
    "Ratio",
    "Ratio"
  ),
  Description = c(
    "Measures income in dollars. Has a true zero (no income).",
    "Measures income per person. Has a true zero.",
    "Number of households paying more than 50% of income in rent.",
    "Number of households paying between 30-35% of income in rent.",
    "Number of households earning less than $10,000.",
    "Number of households earning between $50,000 and $59,999.",
    "Number of households earning between $100,000 and $124,999.",
    "Total population count, which has a true zero (no population)."
  ),
  stringsAsFactors = FALSE
)

# Output the table
kable(features_data, format = "markdown", caption = "Scale of Measurement of Features and Descriptions") %>% 
  kable_styling(full_width = FALSE, font_size = 8.5)
```

### Measures for Similarity/Distance

For clustering analysis, various measures of similarity or distance can be employed based on the features used. The following measures are particularly relevant:

  - **Euclidean Distance:** This is the most widely used distance measure, calculated as the straight-line distance between points in a multi-dimensional space. It is especially effective for continuous numerical data such as income or population figures, where the relationships between data points can be interpreted geometrically. Euclidean distance captures the direct linear relationship between observations, making it intuitive and straightforward for visualizing proximity in clustering contexts. [3]
  - **Manhattan Distance:** This measure calculates the distance between two points by summing the absolute differences of their coordinates. Manhattan distance is useful when dealing with outliers or when the scale of measurement varies among features. It reflects a grid-like path, which can be advantageous in scenarios where a more robust metric against extreme values is required. In urban environments, for example, it mirrors the layout of streets. [4]
  - **Standardization/Normalization:** When features exhibit wide ranges, normalizing the data before applying distance measures is beneficial. This ensures that each feature contributes equally to the distance calculation, preventing features with larger scales from disproportionately influencing results. [5]

In this analysis, a combination of standardized/normalized distance and Euclidean distance will be utilized. The data will first be normalized to ensure that each feature contributes equally to the distance calculation. The choice of Euclidean distance is justified by its prevalence and effectiveness for income and population data, which typically exhibit continuous numerical characteristics. It provides a clear and meaningful way to measure similarity between counties based on economic and demographic factors.

## Modeling
### Normalization
Normalization is essential for standardizing features on a similar scale, enabling meaningful comparisons across variables and preventing features with larger ranges or counts from dominating the analysis—especially in clustering algorithms. Given the wide range of values in the dataset, it was necessary to normalize the numerical features before proceeding with clustering or further analysis.

The normalization was based on the total population of each county, and for each numerical column, a corresponding normalized column was created. A new dataset was then constructed, retaining either the normalized or original version of each feature, depending on its relevance. The following features were kept as not normalized:

  - **county_name:** A categorical variable representing the county's name. Since normalization is typically applied to numerical data, this feature was excluded from the process.
  - **total_pop:** This variable was used as the basis for normalization. Normalizing it would not be meaningful as it serves as the denominator for other variables.
  - **median_income:** Already an average measure of income at the county level, this feature did not require normalization because it provides a direct summary of income status rather than a count or proportion.
  - **income_per_capita:** Similar to median income, this statistic reflects income averaged per individual and does not need normalization, as it is already scaled relative to the population.
 
### Cluster Analysis
#### K-Means Clustering

I tried to do both normalization and standardization to see if one method was better than the other. Whichever you want to use, you can just switch the eval from FALSE to TRUE, or remove the eval all together. 
```{r standardization, echo = FALSE, eval = FALSE}
# Automatically select only numeric columns
numeric_data <- wealthincomedata %>%
  select_if(is.numeric)

# Standardization the numeric columns in wealthincomedata based on total_pop
dataplusstandardization <- as.data.frame(scale(numeric_data))

# View the first few rows of the standardization data
head(dataplusstandardization)
```

```{r normalization, echo = FALSE, eval = FALSE}
# Normalize the numeric columns in wealthincomedata based on total_pop
dataplusnormalization <- wealthincomedata %>%
  mutate(across(-county_name, ~ . / total_pop, .names = "norm_{.col}"))

# View the first few rows of the normalized data
# head(dataplusnormalization)

# Reduce dataset with normalization to only be the necessary normalization columns for clustering.
wealthincomecluster <- dataplusnormalization %>% 
  select(county_name,
         norm_confirmed_cases,
         norm_deaths,
         total_pop,
         median_income,
         income_per_capita,
         norm_rent_burden_not_computed,
         norm_rent_over_50_percent,
         norm_rent_40_to_50_percent, 
         norm_rent_35_to_40_percent,
         norm_rent_30_to_35_percent,
         norm_rent_25_to_30_percent, 
         norm_rent_20_to_25_percent,
         norm_rent_15_to_20_percent,
         norm_rent_10_to_15_percent, 
         norm_rent_under_10_percent,
         norm_income_less_10000,	
         norm_income_10000_14999, 
         norm_income_15000_19999,
         norm_income_20000_24999, 
         norm_income_25000_29999, 
         norm_income_30000_34999, 
         norm_income_35000_39999, 
         norm_income_40000_44999, 
         norm_income_45000_49999, 
         norm_income_50000_59999, 
         norm_income_60000_74999, 
         norm_income_75000_99999, 
         norm_income_100000_124999, 
         norm_income_125000_149999, 
         norm_income_150000_199999, 
         norm_income_200000_or_more)

# View the first few rows of the normalized data
# head(wealthincomecluster)
```

Alot of this is scratch work so it may be repetative or messy, so I apologize. In this code I tried to do K-Means with a smaller subset of the data, focusing on more key features to see if that would make a difference in the clustering. 
``` {r , echo = FALSE, eval = FALSE}
# Load necessary libraries
library(tidyverse)
library(factoextra)

# Assuming 'datatexas' is already loaded with your dataset
# Select the relevant features for clustering
wealthincomedata <- datatexas %>%
  select(county_name,
         confirmed_cases,
         deaths,
         median_income,
         income_per_capita,
         rent_under_10_percent,
         rent_25_to_30_percent,
         rent_over_50_percent,
         income_less_than_10k = income_less_10000,
         income_25k_29k = income_25000_29999,
         income_45k_49k = income_45000_49999,
         income_100k_124k = income_100000_124999,
         income_200k_or_more = income_200000_or_more)

# Remove county_name for clustering
data_to_cluster <- wealthincomedata %>%
  select(-county_name)

# Scale the numeric features
scale_numeric <- function(x) {
  x %>% mutate(across(where(is.numeric), 
                     function(y) (y - mean(y, na.rm = TRUE)) / sd(y, na.rm = TRUE)))
}

wealthincomedata_scaled <- scale_numeric(data_to_cluster)

# Perform K-means clustering
set.seed(123)  # for reproducibility
km <- kmeans(wealthincomedata_scaled, centers = 2, nstart = 10)

# Add cluster information to the original data
wealthincomedata_clustered <- wealthincomedata %>%
  mutate(cluster = factor(km$cluster))

# View the clustered data
print(wealthincomedata_clustered)

# Visualize the clusters
ggplot(wealthincomedata_clustered, aes(x = median_income, y = income_per_capita)) + 
  geom_point(aes(color = cluster)) +
  labs(title = "K-means Clustering of Texas Counties by Income and Wealth",
       x = "Median Income",
       y = "Income per Capita")

# Get the centroids and convert to a data frame
centroids <- as.data.frame(km$centers) %>%
  mutate(cluster = factor(1:nrow(km$centers)))

# Rename the columns of centroids to reflect the original features
colnames(centroids) <- c("median_income", "income_per_capita", "rent_under_10_percent",
                          "rent_25_to_30_percent", "rent_over_50_percent", 
                          "income_less_than_10k", "income_25k_29k", 
                          "income_45k_49k", "income_100k_124k", 
                          "income_200k_or_more", "cluster")

# Plot the clusters and the centroids
ggplot() +
  geom_point(data = wealthincomedata_clustered, 
             aes(x = median_income, y = income_per_capita, color = cluster)) +
  geom_point(data = centroids, 
             aes(x = median_income, y = income_per_capita), 
             color = "black", size = 5, shape = 3) +  # shape = 3 is an 'X' shape
  labs(title = "Cluster Centroids", 
       x = "Median Income", 
       y = "Income per Capita")

# Check the cluster centers
print(km$centers)

# Visualize clustering
fviz_cluster(km, data = wealthincomedata_scaled, geom = "point", ellipse.type = "norm")

``` 

This is similar to the code above but in this I think a new value for the rent and income categories is created to represent where each county falls based on the combination of all of the different categories. 
``` {r step, echo = FALSE, eval = FALSE}
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(reshape2)
library(ggcorrplot)

# Select relevant features
wealthincomedata <- datatexas %>% 
  select(county_name,
         confirmed_cases,
         deaths,
         median_income,
         income_per_capita,
         starts_with("rent"),
         starts_with("income"))

# Create composite indices for income and rent
wealthincomedata <- wealthincomedata %>%
  mutate(income_index = rowMeans(select(., starts_with("income")), na.rm = TRUE),
         rent_index = rowMeans(select(., starts_with("rent")), na.rm = TRUE))

# Standardize features (excluding total population)
wealthincomedata_scaled <- scale(select(wealthincomedata, income_index, rent_index, confirmed_cases, deaths))

# Perform clustering on selected features
set.seed(123) # For reproducibility
km <- kmeans(wealthincomedata_scaled, centers = 3, nstart = 10)

# Add clusters to data
wealthincomedata <- wealthincomedata %>% mutate(cluster = factor(km$cluster))

# Summary statistics of clusters
cluster_summary <- wealthincomedata %>%
  group_by(cluster) %>%
  summarise(
    avg_confirmed_cases = mean(confirmed_cases, na.rm = TRUE),
    avg_deaths = mean(deaths, na.rm = TRUE),
    avg_median_income = mean(median_income, na.rm = TRUE),
    avg_income_per_capita = mean(income_per_capita, na.rm = TRUE),
    avg_rent_index = mean(rent_index, na.rm = TRUE),
  )

print(cluster_summary)

# Visualize confirmed cases by cluster
ggplot(wealthincomedata, aes(x = cluster, y = confirmed_cases, fill = cluster)) +
  geom_boxplot() +
  labs(title = "Confirmed COVID Cases by Cluster",
       x = "Cluster",
       y = "Confirmed Cases") +
  theme_minimal()

# Visualize deaths by cluster
ggplot(wealthincomedata, aes(x = cluster, y = deaths, fill = cluster)) +
  geom_boxplot() +
  labs(title = "COVID Deaths by Cluster",
       x = "Cluster",
       y = "Deaths") +
  theme_minimal()

# Visualize average income by cluster
ggplot(cluster_summary, aes(x = cluster, y = avg_median_income, fill = cluster)) +
  geom_bar(stat = "identity") +
  labs(title = "Average Median Income by Cluster",
       x = "Cluster",
       y = "Average Median Income") +
  theme_minimal()

# Visualize average rent burden by cluster
ggplot(cluster_summary, aes(x = cluster, y = avg_rent_index, fill = cluster)) +
  geom_bar(stat = "identity") +
  labs(title = "Average Rent Index by Cluster",
       x = "Cluster",
       y = "Average Rent Index") +
  theme_minimal()

# Scatter plot to visualize clusters
ggplot(wealthincomedata, aes(x = income_index, y = confirmed_cases, color = cluster)) +
  geom_point(size = 3, alpha = 0.7) +
  labs(title = "Scatter Plot of Income Index vs Confirmed Cases",
       x = "Income Index",
       y = "Confirmed Cases") +
  theme_minimal() +
  scale_color_discrete(name = "Cluster")

# Optional: Correlation heatmap of numeric features
correlation_matrix <- cor(select(wealthincomedata, confirmed_cases, deaths, median_income, income_per_capita, rent_index))
ggcorrplot(correlation_matrix, lab = TRUE, title = "Correlation Heatmap")
``` 
  
I honestly don't remember what this code does exactly. Its running an error so I think I may just have moved on, but if its useful then youre welcome to look at it. 
``` {r step 0, echo = FALSE, eval = FALSE}
# Load necessary libraries
library(dbscan)
library(GGally)
library(ggplot2)
library(dplyr)

# Assuming 'dataplusstandardization' is your dataset
# Scale your dataset if needed (optional, depending on your data)
dataplusstandardization_scaled <- scale(dataplusstandardization)

# Rename columns to V1 and V2 for compatibility
colnames(dataplusstandardization_scaled) <- c("V1", "V2")

# Add an artificial outlier for demonstration
dataplusstandardization_outlier <- rbind(dataplusstandardization_scaled, c(10, 0))

# Perform k-means clustering on the dataset with outlier
km <- kmeans(dataplusstandardization_outlier, centers = 4, nstart = 10)
dataplusstandardization_outlier_km <- as.data.frame(dataplusstandardization_outlier) %>%
  mutate(cluster = factor(km$cluster))
centroids <- as.data.frame(km$centers) %>%
  mutate(cluster = factor(1:nrow(km$centers)))

# Visualize clusters with outlier
ggplot(dataplusstandardization_outlier_km, aes(x = V1, y = V2, color = cluster)) + 
  geom_point() +
  geom_point(data = centroids, aes(x = V1, y = V2, color = cluster), shape = 3, size = 5)

# Calculate Local Outlier Factor
lof <- lof(dataplusstandardization_outlier, minPts = 10)

# Visualize LOF values
ggplot(as.data.frame(dataplusstandardization_outlier) %>% mutate(lof = lof), 
       aes(V1, V2, color = lof)) +
  geom_point() + 
  scale_color_gradient(low = "gray", high = "red")

# Plot sorted LOF values
ggplot(tibble(index = seq_along(lof), lof = sort(lof)), 
       aes(index, lof)) +
  geom_line() +
  geom_hline(yintercept = 1, color = "red", linetype = 2)

# Identify outliers based on LOF values
outlier_threshold <- 2
ggplot(as.data.frame(dataplusstandardization_outlier) %>% 
         mutate(outlier = lof >= outlier_threshold), 
       aes(V1, V2, color = outlier)) +
  geom_point()

# Remove outliers
dataplusstandardization_clean <- dataplusstandardization_outlier %>% 
  filter(lof < outlier_threshold)

# Perform k-means clustering on the cleaned dataset
km_clean <- kmeans(dataplusstandardization_clean, centers = 4, nstart = 10)
dataplusstandardization_clean_km <- as.data.frame(dataplusstandardization_clean) %>%
  mutate(cluster = factor(km_clean$cluster))
centroids_clean <- as.data.frame(km_clean$centers) %>%
  mutate(cluster = factor(1:nrow(km_clean$centers)))

# Visualize clusters without outliers
ggplot(dataplusstandardization_clean_km, aes(x = V1, y = V2, color = cluster)) + 
  geom_point() +
  geom_point(data = centroids_clean, aes(x = V1, y = V2, color = cluster), shape = 3, size = 5)

```
  

I think this is the clustering done on the original subset that has all of the columns of rent and income categories. This should follow the method Dr. Hahsler did in the textbook. 
``` {r step 1, echo = FALSE, eval = FALSE}
library(tidyverse)
library(cluster)
library(factoextra)

# Remove non-numeric columns (e.g., county_name)
numeric_data <- wealthincomedata %>%
  select(-county_name)

# Scale the numeric data
scale_numeric <- function(x) {
  x |> mutate(across(where(is.numeric), 
                     function(y) (y - mean(y, na.rm = TRUE)) / sd(y, na.rm = TRUE)))
}

wealthincomedata_scaled <- numeric_data |> 
  scale_numeric()

# Perform K-means clustering
set.seed(123)  # For reproducibility
km <- kmeans(wealthincomedata_scaled, centers = 2, nstart = 10)

# Add the cluster column to the original dataset
wealthincomedata_clustered <- wealthincomedata |> 
  mutate(cluster = factor(km$cluster))

# Plot the clusters
ggplot(wealthincomedata_clustered, aes(x = median_income, y = income_per_capita)) + 
  geom_point(aes(color = cluster)) +
  labs(title = "K-Means Clustering Results", x = "Median Income", y = "Income Per Capita") +
  theme_minimal()

# Extract centroids and unscale them
unscale <- function(x, original_data) {
  if (ncol(x) != ncol(original_data))
    stop("Function needs matching columns!")
  x * matrix(apply(original_data, MARGIN = 2, sd, na.rm = TRUE), 
             byrow = TRUE, nrow = nrow(x), ncol = ncol(x)) + 
    matrix(apply(original_data, MARGIN = 2, mean, na.rm = TRUE), 
           byrow = TRUE, nrow = nrow(x), ncol = ncol(x))
}

# Extract centroids and unscale them
centroids <- km$centers %>% 
  unscale(original_data = numeric_data) %>% 
  as_tibble() %>%
  mutate(cluster = factor(1:n()))  # Create a 'cluster' column for plotting

# Plot centroids on the same scatter plot
ggplot(wealthincomedata_clustered, aes(x = median_income, y = income_per_capita)) + 
  geom_point(aes(color = cluster)) +
  geom_point(data = centroids, aes(x = median_income, y = income_per_capita, color = cluster), 
             shape = 3, size = 5) +
  labs(title = "K-Means Clustering with Centroids", x = "Median Income", y = "Income Per Capita") +
  theme_minimal()

# Visualize clustering
fviz_cluster(km, data = wealthincomedata_scaled, geom = "point", ellipse.type = "norm")

# Check the size of each cluster
cluster_sizes <- table(wealthincomedata_clustered$cluster)
print(cluster_sizes)
``` 
  
This is a few methods to determine the number of clusters to use. Most of the charts show that 1 or 2 clusters is ideal, but I don't know that this makes sense.
``` {r step 2, echo = FALSE, eval = FALSE}
# Load necessary libraries
library(tidyverse)
library(cluster)
library(factoextra)

# Assuming your data is stored in 'dataplusstandardization'

# Set the maximum number of clusters to evaluate
max_k <- 10

# --- Elbow Method ---
wss <- numeric(max_k)

for (k in 1:max_k) {
  set.seed(123) # For reproducibility
  kmeans_result <- kmeans(dataplusstandardization, centers = k, nstart = 10)
  wss[k] <- kmeans_result$tot.withinss
}

# Plot the Elbow curve
plot(1:max_k, wss, type = "b", pch = 19, xlab = "Number of Clusters (k)", 
     ylab = "Total Within-Cluster Sum of Squares (WSS)", 
     main = "Elbow Method for Optimal k")


# --- Silhouette Method ---
silhouette_scores <- numeric(max_k)

for (k in 2:max_k) {
  set.seed(123) # For reproducibility
  km <- kmeans(dataplusstandardization, centers = k, nstart = 10)
  ss <- silhouette(km$cluster, dist(dataplusstandardization))
  silhouette_scores[k] <- mean(ss[, 3]) # Average silhouette width
}

# Plot the Silhouette scores
plot(2:max_k, silhouette_scores[2:max_k], type = "b", pch = 19, 
     xlab = "Number of Clusters (k)", 
     ylab = "Average Silhouette Width", 
     main = "Silhouette Method for Optimal k")


# --- Gap Statistic ---
set.seed(123) # For reproducibility
gap_stat <- clusGap(dataplusstandardization, FUN = kmeans, nstart = 10, K.max = max_k, B = 50)
fviz_gap_stat(gap_stat)

``` 
  
This is another attempt at K-Means. It's running an error right now so I cant remember exactly what I was trying to do here. 
```{r k-means clustering 2, echo = FALSE, eval = FALSE}
km <- kmeans(dataplusstandardization, centers = 4, nstart = 10)

# Add the cluster column to the dataset using cbind
data_clustered <- cbind(dataplusstandardization, cluster = factor(km$cluster))

# Convert it to a data frame if it's not already one
data_clustered <- as.data.frame(data_clustered)

data_clustered

ggplot(data_clustered, aes(x = x, y = y)) + 
  geom_point(aes(color = cluster))
```

This was the initial K-Means clustering I did. First I selected the features to include in the clustering dataset. 
```{r k-means clustering, echo = FALSE, eval = FALSE}
# Select the columns to cluster.
clustering <- wealthincomecluster %>% 
  select(norm_confirmed_cases,
         norm_deaths,
         median_income,
         income_per_capita,
         norm_rent_burden_not_computed,
         norm_rent_over_50_percent,
         norm_rent_40_to_50_percent, 
         norm_rent_35_to_40_percent,
         norm_rent_30_to_35_percent,
         norm_rent_25_to_30_percent, 
         norm_rent_20_to_25_percent,
         norm_rent_15_to_20_percent,
         norm_rent_10_to_15_percent, 
         norm_rent_under_10_percent,
         norm_income_less_10000,	
         norm_income_10000_14999, 
         norm_income_15000_19999,
         norm_income_20000_24999, 
         norm_income_25000_29999, 
         norm_income_30000_34999, 
         norm_income_35000_39999, 
         norm_income_40000_44999, 
         norm_income_45000_49999, 
         norm_income_50000_59999, 
         norm_income_60000_74999, 
         norm_income_75000_99999, 
         norm_income_100000_124999, 
         norm_income_125000_149999, 
         norm_income_150000_199999, 
         norm_income_200000_or_more)
```

This is a correlation analysis to see if any of the features are highly correlated and reduntant.
```{r correlation analysis, echo = FALSE, eval =}
# Load necessary libraries
library(ggplot2)
library(reshape2)
library(corrplot)

# Assuming 'wealthincomecluster' is your data frame
# Select the relevant columns to analyze
columns_to_analyze <- wealthincomecluster %>%
  select(norm_confirmed_cases, norm_deaths, norm_median_income, norm_income_per_capita,
         norm_rent_burden_not_computed, norm_rent_over_50_percent, 
         norm_rent_40_to_50_percent, norm_rent_35_to_40_percent,
         norm_rent_30_to_35_percent, norm_rent_25_to_30_percent, 
         norm_rent_20_to_25_percent, norm_rent_15_to_20_percent,
         norm_rent_10_to_15_percent, norm_rent_under_10_percent,
         norm_income_less_10000, norm_income_10000_14999, norm_income_15000_19999,
         norm_income_20000_24999, norm_income_25000_29999, norm_income_30000_34999, 
         norm_income_35000_39999, norm_income_40000_44999, norm_income_45000_49999,
         norm_income_50000_59999, norm_income_60000_74999, norm_income_75000_99999, 
         norm_income_100000_124999, norm_income_125000_149999, norm_income_150000_199999, 
         norm_income_200000_or_more)

# Compute the correlation matrix
correlation_matrix <- cor(columns_to_analyze, use = "complete.obs")

# Set a threshold for high correlation (e.g., 0.8)
threshold <- 0.8

# Find pairs of features with absolute correlation above the threshold
high_correlations <- which(abs(correlation_matrix) > threshold, arr.ind = TRUE)

# Filter out diagonal elements (self-correlations)
high_correlations <- high_correlations[high_correlations[, 1] != high_correlations[, 2], ]

# Print the highly correlated feature pairs
if(nrow(high_correlations) > 0) {
  for(i in 1:nrow(high_correlations)) {
    feature_1 <- colnames(correlation_matrix)[high_correlations[i, 1]]
    feature_2 <- colnames(correlation_matrix)[high_correlations[i, 2]]
    correlation_value <- correlation_matrix[high_correlations[i, 1], high_correlations[i, 2]]
    print(paste(feature_1, "and", feature_2, "have a correlation of", round(correlation_value, 2)))
  }
} else {
  print("No features have a correlation higher than the threshold.")
}
```

This is my original K-Means clustering calculation and display. 
```{r k-means clustering display, echo = FALSE, fig.width = 6, fig.height = 4, fig.align = 'center', fig.cap = "K-Means Clustering, Wealth and Income"}
# Perform k-means clustering with a chosen number of clusters
kmeans_result <- kmeans(dataplusstandardization, centers = 3, nstart = 15)

# Visualize k-means clusters
fviz_cluster(kmeans_result, data = clustering) +
  labs(title = NULL) # Remove the title

# Add the cluster assignments to the original dataset
wealthincomeclusterkmean <- wealthincomecluster
wealthincomeclusterkmean$cluster <- kmeans_result$cluster

# Display counties and their cluster assignments
county_clusters <- data.frame(county_name = wealthincomeclusterkmean$county_name, 
                              cluster = wealthincomeclusterkmean$cluster)

# View the first few rows
# head(county_clusters)
```

This is my original determination of number of clusters. 
```{r number of clusters, echo = FALSE, fig.width = 6, fig.height = 4, fig.align = 'center', fig.cap = "K-Means Optimal Number of Clusters"}
# Use the Elbow method to determine the optimal number of clusters
fviz_nbclust(clustering, kmeans, method = "wss") +
  labs(title = NULL) # Remove the title
```

This is where I calculated the basic statistics for each cluster
```{r basic statistics normalized, echo = FALSE}
# Calculate basic statistics
statstable <- wealthincomecluster %>%
  summarise(
    mean_median_income = mean(median_income, na.rm = TRUE),
    sd_median_income = sd(median_income, na.rm = TRUE),
    min_median_income = min(median_income, na.rm = TRUE),
    max_median_income = max(median_income, na.rm = TRUE),
    
    mean_income_per_capita = mean(income_per_capita, na.rm = TRUE),
    sd_income_per_capita = sd(income_per_capita, na.rm = TRUE),
    min_income_per_capita = min(income_per_capita, na.rm = TRUE),
    max_income_per_capita = max(income_per_capita, na.rm = TRUE),
    
    mean_rent_over_50_percent = mean(norm_rent_over_50_percent, na.rm = TRUE),
    sd_rent_over_50_percent = sd(norm_rent_over_50_percent, na.rm = TRUE),
    min_rent_over_50_percent = min(norm_rent_over_50_percent, na.rm = TRUE),
    max_rent_over_50_percent = max(norm_rent_over_50_percent, na.rm = TRUE),
    
    mean_rent_30_to_35_percent = mean(norm_rent_30_to_35_percent, na.rm = TRUE),
    sd_rent_30_to_35_percent = sd(norm_rent_30_to_35_percent, na.rm = TRUE),
    min_rent_30_to_35_percent = min(norm_rent_30_to_35_percent, na.rm = TRUE),
    max_rent_30_to_35_percent = max(norm_rent_30_to_35_percent, na.rm = TRUE),
    
    mean_income_less_10000 = mean(norm_income_less_10000, na.rm = TRUE),
    sd_income_less_10000 = sd(norm_income_less_10000, na.rm = TRUE),
    min_income_less_10000 = min(norm_income_less_10000, na.rm = TRUE),
    max_income_less_10000 = max(norm_income_less_10000, na.rm = TRUE),
    
    mean_income_50000_59999 = mean(norm_income_50000_59999, na.rm = TRUE),
    sd_income_50000_59999 = sd(norm_income_50000_59999, na.rm = TRUE),
    min_income_50000_59999 = min(norm_income_50000_59999, na.rm = TRUE),
    max_income_50000_59999 = max(norm_income_50000_59999, na.rm = TRUE),
    
    mean_income_100000_124999 = mean(norm_income_100000_124999, na.rm = TRUE),
    sd_income_100000_124999 = sd(norm_income_100000_124999, na.rm = TRUE),
    min_income_100000_124999 = min(norm_income_100000_124999, na.rm = TRUE),
    max_income_100000_124999 = max(norm_income_100000_124999, na.rm = TRUE),
    
    mean_total_pop = mean(total_pop, na.rm = TRUE),
    sd_total_pop = sd(total_pop, na.rm = TRUE),
    min_total_pop = min(total_pop, na.rm = TRUE),
    max_total_pop = max(total_pop, na.rm = TRUE)
  )

# Prepare a summary table with descriptions and formatting for non-scientific notation and 4 decimal places
statstable_final <- data.frame(
  Feature = c(
    "median_income",
    "income_per_capita",
    "rent_over_50_percent",
    "rent_30_to_35_percent",
    "income_less_10000",	
    "income_50000_59999", 
    "income_100000_124999", 
    "total_pop"
  ),
  Mean = format(c(
    statstable$mean_median_income,
    statstable$mean_income_per_capita,
    statstable$mean_rent_over_50_percent,
    statstable$mean_rent_30_to_35_percent,
    statstable$mean_income_less_10000,
    statstable$mean_income_50000_59999,
    statstable$mean_income_100000_124999,
    statstable$mean_total_pop
  ), scientific = FALSE, nsmall = 4),
  
  Std_Dev = format(c(
    statstable$sd_median_income,
    statstable$sd_income_per_capita,
    statstable$sd_rent_over_50_percent,
    statstable$sd_rent_30_to_35_percent,
    statstable$sd_income_less_10000,
    statstable$sd_income_50000_59999,
    statstable$sd_income_100000_124999,
    statstable$sd_total_pop
  ), scientific = FALSE, nsmall = 4),
  
  Min = format(c(
    statstable$min_median_income,
    statstable$min_income_per_capita,
    statstable$min_rent_over_50_percent,
    statstable$min_rent_30_to_35_percent,
    statstable$min_income_less_10000,
    statstable$min_income_50000_59999,
    statstable$min_income_100000_124999,
    statstable$min_total_pop
  ), scientific = FALSE, nsmall = 4),
  
  Max = format(c(
    statstable$max_median_income,
    statstable$max_income_per_capita,
    statstable$max_rent_over_50_percent,
    statstable$max_rent_30_to_35_percent,
    statstable$max_income_less_10000,
    statstable$max_income_50000_59999,
    statstable$max_income_100000_124999,
    statstable$max_total_pop
  ), scientific = FALSE, nsmall = 4)
)

# Output the table
kable(statstable_final, format = "markdown", caption = "Table of Features and Basic Statistics, Normalized") %>% 
  kable_styling(full_width = FALSE, font_size = 8.5)

```

Similar to above.
```{r summary table cluster 1, echo = FALSE}
# Filter data for cluster 1
cluster1 <- wealthincomeclusterkmean %>% 
  filter(cluster == "1")

# Get the numeric columns in cluster 1
numeric_cols <- cluster1 %>% select(where(is.numeric))

# Initialize an empty data frame to store the results
summary_df <- data.frame(Feature = character(),
                         Mean = numeric(),
                         SD = numeric(),
                         Min = numeric(),
                         Max = numeric(),
                         stringsAsFactors = FALSE)

# Loop over each column and calculate the summary statistics
for (colname in colnames(numeric_cols)) {
  # Calculate summary statistics
  mean_val <- mean(numeric_cols[[colname]], na.rm = TRUE)
  sd_val <- sd(numeric_cols[[colname]], na.rm = TRUE)
  min_val <- min(numeric_cols[[colname]], na.rm = TRUE)
  max_val <- max(numeric_cols[[colname]], na.rm = TRUE)
  
  # Round values to 4 decimal places
  mean_val <- round(mean_val, 4)
  sd_val <- round(sd_val, 4)
  min_val <- round(min_val, 4)
  max_val <- round(max_val, 4)
  
  # Create a new row and append it to summary_df
  summary_df <- rbind(summary_df, 
                      data.frame(Feature = colname,
                                 Mean = format(mean_val, nsmall = 4, scientific = FALSE),
                                 SD = format(sd_val, nsmall = 4, scientific = FALSE),
                                 Min = format(min_val, nsmall = 4, scientific = FALSE),
                                 Max = format(max_val, nsmall = 4, scientific = FALSE)))
}

# Display the summary table
library(knitr)
kable(summary_df, col.names = c("Feature", "Mean", "SD", "Min", "Max"),
      caption = "Summary Statistics for Cluster 1") %>%
  kable_styling(full_width = FALSE)
```

Similar to above.
```{r summary table cluster 2, echo = FALSE}
# Filter data for cluster 2
cluster2 <- wealthincomeclusterkmean %>% 
  filter(cluster == "2")

# Get the numeric columns in cluster 2
numeric_cols <- cluster2 %>% select(where(is.numeric))

# Initialize an empty data frame to store the results
summary_df <- data.frame(Feature = character(),
                         Mean = numeric(),
                         SD = numeric(),
                         Min = numeric(),
                         Max = numeric(),
                         stringsAsFactors = FALSE)

# Loop over each column and calculate the summary statistics
for (colname in colnames(numeric_cols)) {
  # Calculate summary statistics
  mean_val <- mean(numeric_cols[[colname]], na.rm = TRUE)
  sd_val <- sd(numeric_cols[[colname]], na.rm = TRUE)
  min_val <- min(numeric_cols[[colname]], na.rm = TRUE)
  max_val <- max(numeric_cols[[colname]], na.rm = TRUE)
  
  # Round values to 4 decimal places
  mean_val <- round(mean_val, 4)
  sd_val <- round(sd_val, 4)
  min_val <- round(min_val, 4)
  max_val <- round(max_val, 4)
  
  # Create a new row and append it to summary_df
  summary_df <- rbind(summary_df, 
                      data.frame(Feature = colname,
                                 Mean = format(mean_val, nsmall = 4, scientific = FALSE),
                                 SD = format(sd_val, nsmall = 4, scientific = FALSE),
                                 Min = format(min_val, nsmall = 4, scientific = FALSE),
                                 Max = format(max_val, nsmall = 4, scientific = FALSE)))
}

# Display the summary table
library(knitr)
kable(summary_df, col.names = c("Feature", "Mean", "SD", "Min", "Max"),
      caption = "Summary Statistics for Cluster 2") %>%
  kable_styling(full_width = FALSE)
```

Similar to above. 
```{r summary table cluster 3, echo = FALSE}
# Filter data for cluster 3
cluster3 <- wealthincomeclusterkmean %>% 
  filter(cluster == "3")

# Get the numeric columns in cluster 3
numeric_cols <- cluster3 %>% select(where(is.numeric))

# Initialize an empty data frame to store the results
summary_df <- data.frame(Feature = character(),
                         Mean = numeric(),
                         SD = numeric(),
                         Min = numeric(),
                         Max = numeric(),
                         stringsAsFactors = FALSE)

# Loop over each column and calculate the summary statistics
for (colname in colnames(numeric_cols)) {
  # Calculate summary statistics
  mean_val <- mean(numeric_cols[[colname]], na.rm = TRUE)
  sd_val <- sd(numeric_cols[[colname]], na.rm = TRUE)
  min_val <- min(numeric_cols[[colname]], na.rm = TRUE)
  max_val <- max(numeric_cols[[colname]], na.rm = TRUE)
  
  # Round values to 4 decimal places
  mean_val <- round(mean_val, 4)
  sd_val <- round(sd_val, 4)
  min_val <- round(min_val, 4)
  max_val <- round(max_val, 4)
  
  # Create a new row and append it to summary_df
  summary_df <- rbind(summary_df, 
                      data.frame(Feature = colname,
                                 Mean = format(mean_val, nsmall = 4, scientific = FALSE),
                                 SD = format(sd_val, nsmall = 4, scientific = FALSE),
                                 Min = format(min_val, nsmall = 4, scientific = FALSE),
                                 Max = format(max_val, nsmall = 4, scientific = FALSE)))
}

# Display the summary table
library(knitr)
kable(summary_df, col.names = c("Feature", "Mean", "SD", "Min", "Max"),
      caption = "Summary Statistics for Cluster 3") %>%
  kable_styling(full_width = FALSE)
```

#### Hierarchical Clustering
I did not get this far and this was previous work I had. I only really was focused on K-Means
```{r hierarchical clustering, echo = FALSE}
# Generate example data
set.seed(123)  # For reproducibility

# Select numeric columns
numeric_cols <- wealthincomecluster %>% select(where(is.numeric))

# Compute the distance matrix
distance_matrix <- dist(wealthincomecluster, method = "euclidean")

# Perform hierarchical clustering
hc <- hclust(distance_matrix, method = "complete")

# Plot the dendrogram
plot(hc, main = "Hierarchical Clustering Dendrogram", xlab = "Observations", ylab = "Height")

# Cut the dendrogram to create clusters
clusters <- cutree(hc, k = 3)

# Add the cluster assignments to the original dataset
wealthincomeclusterhierarchical <- wealthincomecluster
wealthincomeclusterhierarchical$cluster <- as.factor(clusters)

# Perform PCA on the numeric columns
pca_result <- prcomp(numeric_cols, scale. = TRUE)

# Add the first two principal components to the dataset
wealthincomeclusterhierarchical$PC1 <- pca_result$x[, 1]
wealthincomeclusterhierarchical$PC2 <- pca_result$x[, 2]

# Visualize clusters using the first two principal components
ggplot(wealthincomeclusterhierarchical, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point(size = 3) +
  labs(title = "Hierarchical Clustering Result (PCA)") +
  theme_minimal()
```



Use unsupervised evaluation to describe and compare the clusterings and the clusters (some visual methods would be good). [10]

Identify a feature you could use as the ground truth to perform supervised evaluation. Compare the clusterings using this method. [10]

## Evaluation
Describe your results. What recommendations can you formulate based on the clustering results? How do these recommendations relate to the ones already presented in report 1?

What findings are the most interesting to your stakeholder? 

## List of References

[1] “Covid-19,” NFID, [https://www.nfid.org/infectious-diseases/covid-19/](https://www.nfid.org/infectious-diseases/covid-19/) (accessed Oct. 8, 2024).

[2] Northwestern Medicine, “Covid-19 pandemic timeline,” Northwestern Medicine, [https://www.nm.org/healthbeat/medical-advances/new-therapies-and-drug-trials/covid-19-pandemic-timeline](https://www.nm.org/healthbeat/medical-advances/new-therapies-and-drug-trials/covid-19-pandemic-timeline) (accessed Oct. 8, 2024).

[3] “10.1 - hierarchical clustering,” 10.1 - Hierarchical Clustering | STAT 555, [https://online.stat.psu.edu/stat555/node/85/#:~:text=For%20most%20common%20hierarchical%20clustering,when%20they%20are%20perfectly%20correlated.](https://online.stat.psu.edu/stat555/node/85/#:~:text=For%20most%20common%20hierarchical%20clustering,when%20they%20are%20perfectly%20correlated.) (accessed Oct. 23, 2024). 

[4] “Manhattan distance,” Wikipedia, [https://simple.wikipedia.org/wiki/Manhattan_distance](https://simple.wikipedia.org/wiki/Manhattan_distance) (accessed Oct. 23, 2024). 

[5] A. Jain, “Normalization and standardization of Data,” Medium,  
[https://medium.com/@abhishekjainindore24/normalization-and-standardization-of-data-408810a88307](https://medium.com/@abhishekjainindore24/normalization-and-standardization-of-data-408810a88307) (accessed Oct. 23, 2024).